# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T2dovHihTFnJBSmm2pt-kp-jKEP6sUuN
"""

from google.colab import drive
drive.mount('/content/gdrive/')

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import wandb

DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# CNN class
class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.pool = nn.MaxPool2d(2, 2)
    self.conv1 = nn.Conv2d(3, 18, 5)
    self.conv2 = nn.Conv2d(18, 36, 5)
    self.fc_for_q1 = nn.Linear(36 * 5 * 5, 10)
    self.fc_for_q2 = nn.Linear(36 * 24 * 24, 10)
    self.fc_for_q3_op1 = nn.Linear(36 * 28 * 28, 10)
    self.fc_for_q3_op2 = nn.Linear(18, 10)


  def forward(self, x):
    x = self.Q1_non_linear_forward(x)
    # x = self.Q2_linear_forward(x)
    # x = self.Q3_option1_forward(x)
    # x = self.Q3_option2_forward(x)
    return x
  
  def Q1_non_linear_forward(self, x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = torch.flatten(x, 1)  # flatten all dimensions except batch
    x = F.relu(self.fc_for_q1(x))
    return x

  def Q2_linear_forward(self, x):
    # Remove all non linearities
    x = self.conv1(x)
    x = self.conv2(x)
    x = torch.flatten(x, 1)  # flatten all dimensions except batch
    x = self.fc_for_q2(x)
    return x
  
  def Q3_option1_forward(self, x):
    x = F.relu(self.conv1(x))
    x = torch.flatten(x, 1)  # flatten all dimensions except batch
    x = F.relu(self.fc_for_q3_op1(x))
    return x
  
  def Q3_option2_forward(self, x):
    x = F.relu(self.conv1(x))
    x = torch.mean(x, dim=[2, 3]) # global average pooling on each channel
    x = torch.flatten(x, 1)  # flatten all dimensions except batch
    x = F.relu(self.fc_for_q3_op2(x))
    return x

def load_data_and_normalize():
  transform = transforms.Compose([transforms.ToTensor(),
                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                          download=True, transform=transform)

  testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                          download=True, transform=transform)

  return trainset, testset

def create_loader(dataset, batch_size=4):
  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, 
                                       shuffle=True, num_workers=2)
  return loader

def test_cnn_module(cnn_net, dataset, batch_size):
  correct = 0
  total = 0
  loss = 0.0
  num_of_baches = 0
  criterion = nn.CrossEntropyLoss()
  loader = create_loader(dataset, batch_size)

  with torch.no_grad():
    for data in loader:
      images, labels = data[0].to(DEVICE), data[1].to(DEVICE)
      # calculate outputs by running images through the network
      outputs = cnn_net(images)
      # Update loss variables
      loss += criterion(outputs, labels).item()
      num_of_baches += 1

      # Update accuracy variables
      _, predicted = torch.max(outputs.data, 1)
      correct += (predicted == labels).sum().item()
      total += labels.size(0)


  loss = round(loss / num_of_baches, 3)
  accuracy = round((100 * correct) / total, 3)

  return loss, accuracy

def train_cnn(num_of_epochs, batch_size, learning_rate):

  # Load data and normalize (CIFAR10)
  trainset, testset = load_data_and_normalize()
  trainloader = create_loader(trainset, batch_size)
  testloader = create_loader(testset, batch_size)
  # Initiate CNN
  cnn_net = Net()
  cnn_net = cnn_net.to(DEVICE)

  # Define a Loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(cnn_net.parameters(), lr=learning_rate, momentum=0.9)

  print('Starting training CNN module')

  for epoch in range(num_of_epochs):  # loop over the dataset multiple times

    for i, data in enumerate(trainloader, 0):
      # get the inputs; data is a list of [inputs, labels]
      inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)

      # zero the parameter gradients
      optimizer.zero_grad()

      # forward + backward + optimize
      outputs = cnn_net(inputs)
      loss = criterion(outputs, labels)
      loss.backward()
      optimizer.step()

      # Compute train-loss, train-accuracy, test-loss and test-accuracy if the network every 50 batch
      if i % 50 == 49:
        test_loss, test_acc = test_cnn_module(cnn_net, testset, batch_size)
        train_loss, train_acc = test_cnn_module(cnn_net, trainset, batch_size)
        # wandb.log({'train loss': train_loss, 'train accuracy': train_acc, 'test accuracy': test_acc, 'test loss': test_loss})
        print("epoch={}: train loss={}, train accuracy={}, test loss={}, test accuracy={}".format(epoch, train_loss, train_acc, test_loss, test_acc))

  print('Finished Training CNN module')

project_name = "DL4Images_ex1.Q1"
num_of_epochs = 10
batch_size = 16
lr = 5e-4

# For tester: I censored the wandb api and commented all the WANDB parts so the code be able to work
#wandb.login(relogin=True, key="**********")
#wandb.init(project=project_name,
#           config={
#               "num_of_epochs": num_of_epochs,
#               "batch_size": batch_size,
#               "lr": lr
#           })

train_cnn(num_of_epochs, batch_size, lr)
